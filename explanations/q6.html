<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Explanation for Question q6</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
    </style>
</head>
<body class="bg-gray-100 text-gray-800">
    <div class="container mx-auto p-4 sm:p-6 lg:p-8">
        <div class="bg-white p-6 sm:p-8 rounded-2xl shadow-lg max-w-4xl mx-auto pb-20">
            <h1 class="text-2xl sm:text-3xl font-bold text-gray-900 mb-6">Explanation</h1>
            <div class="prose prose-lg max-w-none">
                <p>This scenario describes a classic production incident following a deployment (a new pipeline version), where data integrity is compromised, specifically resulting in data duplication. The key to solving this is establishing a rigorous forensic process to identify which jobs caused the duplication and why they ran simultaneously, and then fixing the system configuration to prevent recurrence.</p>

                <h3>Step 1: Analyze the Problem and Determine the Hypothesis</h3>
                <ul class="list-disc list-inside space-y-2">
                    <li><strong>Workload:</strong> Streaming ingestion from Pub/Sub to BigQuery tables, likely using Dataflow.</li>
                    <li><strong>Observation:</strong> Pub/Sub source volume is constant, but BigQuery stored data (in specific partitions) doubled (a 100% increase on affected tables).</li>
                    <li><strong>Hypothesis:</strong> The doubling of data implies that data is being written twice. This is highly probable when a new pipeline version is deployed, leading to two concurrent, non-idempotent processes writing the same stream data to the same sink (e.g., the old pipeline version was not properly stopped/drained before the new one started). In streaming systems, preventing duplication requires careful handling of task idempotency and managing concurrent writers.</li>
                </ul>

                <h3>Step 2: Formulate the Investigation and Root Cause Analysis Plan</h3>
                <p>To investigate and fix this, the data engineer must follow a structured approach focusing on the data artifact (BigQuery table) and tracing back to the execution job (Dataflow pipeline).</p>
                <ol class="list-decimal list-inside space-y-2">
                    <li><strong>Verify Data Corruption:</strong> Check the hypothesis by confirming the existence of duplicate rows in the affected BigQuery tables.</li>
                    <li><strong>Trace Write Operations (BigQuery Audit Logs):</strong> Find the jobs responsible for writing the duplicated data to BigQuery. BigQuery Audit Logs (specifically using the BigQueryAuditMetadata format) provide records of TableDataChange events, including the associated job IDs. This links the data corruption directly to the job instance.</li>
                    <li><strong>Identify Pipeline Execution Details (Cloud Monitoring/Logging):</strong> Use the job IDs obtained from the BigQuery Audit Logs to query Cloud Logging or Cloud Monitoring. These services provide visibility into the history and execution status of Dataflow jobs, allowing us to pinpoint the pipeline version, start time, and determine if multiple jobs were running in parallel to the same sink.</li>
                    <li><strong>Resolve the Source of Duplication:</strong> If multiple pipeline versions (old and new) are confirmed to be writing concurrently, the redundant/failing version must be stopped.</li>
                </ol>

                <h3>Step 3: Evaluate the Options</h3>
                <div class="overflow-x-auto">
                    <table class="table-auto w-full mb-4 border border-collapse">
                        <thead>
                            <tr class="bg-gray-200">
                                <th class="px-4 py-2 border text-left">Option</th>
                                <th class="px-4 py-2 border text-left">Critique</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td class="px-4 py-2 border font-bold align-top">A</td>
                                <td class="px-4 py-2 border">Fixes the symptom (deduplication) but not the root cause (concurrent writers). Fails to resolve the underlying bug in the pipeline orchestration.</td>
                            </tr>
                            <tr>
                                <td class="px-4 py-2 border font-bold align-top">B</td>
                                <td class="px-4 py-2 border">Lacks the critical step of using BigQuery Audit Logs to trace the specific write job. Doesn't explicitly stop the continuous source of duplication.</td>
                            </tr>
                            <tr>
                                <td class="px-4 py-2 border font-bold align-top">C</td>
                                <td class="px-4 py-2 border">This option follows the most rigorous troubleshooting path for a production streaming issue caused by a deployment error. It correctly identifies the problem, traces it to the source, and resolves it.</td>
                            </tr>
                            <tr>
                                <td class="px-4 py-2 border font-bold align-top">D</td>
                                <td class="px-4 py-2 border">Skips the necessary forensic investigation required to confirm the cause before executing costly reprocessing and restoration.</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h3>Conclusion</h3>
                <p>The correct sequence of actions involves confirming the data error, using auditing tools to trace the error back to the responsible executing service, and then mitigating the root cause of continuous duplication.</p>
                <p>The best answer is <strong>C. 1. Check for duplicate rows in the BigQuery tables that have the daily partition data size doubled. 2. Check the BigQuery Audit logs to find job IDs. 3. Use Cloud Monitoring to determine when the identified Dataflow jobs started and the pipeline code version. 4. When more than one pipeline ingests data into a table, stop all versions except the latest one.</strong></p>
                <p>This approach ensures:</p>
                <ul class="list-disc list-inside space-y-2">
                    <li><strong>Diagnosis:</strong> Confirming duplication exists.</li>
                    <li><strong>Root Cause Identification:</strong> Using BigQuery Audit Logs and Cloud Monitoring to trace the write operations (TableDataChange events) to find concurrent Dataflow jobs.</li>
                    <li><strong>Resolution:</strong> Stopping the redundant pipeline(s) to halt continuous duplication. (The data cleanup, such as using BigQuery's Time Travel feature, would follow this step, but identifying and stopping the source of the error is the paramount action).</li>
                </ul>
            </div>
        </div>
    </div>
    <footer class="fixed bottom-0 left-0 w-full bg-gray-800 text-white p-4 shadow-lg">
        <div class="container mx-auto flex items-center justify-center">
            <audio controls class="w-full max-w-md">
                <source src="q6.m4a" type="audio/mp4">
                Your browser does not support the audio element.
            </audio>
        </div>
    </footer>
</body>
</html>