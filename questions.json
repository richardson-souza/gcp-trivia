[
  {
    "id": "q1",
    "question": "You are running a BigQuery project using the on-demand billing model, and you are executing a change data capture (CDC) process that ingests data. The CDC process loads 1 GB of data every 10 minutes into a temporary table and performs a merge into a 10 TB target table. This process is resource-intensive and involves significant scanning. You want to explore options to implement a predictable cost model. You need to create a BigQuery reservation based on usage data gathered from BigQuery Monitoring and apply the reservation to the CDC process.",
    "options": [
      "Create a BigQuery reservation for the dataset.",
      "Create a BigQuery reservation for the job.",
      "Create a BigQuery reservation for the service account running the job.",
      "Create a BigQuery reservation for the project."
    ],
    "correct_answers": ["Create a BigQuery reservation for the project."],
    "categories": ["Data Ingestion", "Data Processing", "Analytics"],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "For real-time, large-scale scenarios, <strong>Cloud Pub/Sub</strong> is the ideal ingestion service due to its scalability and durability. <strong>Cloud Dataflow</strong> provides a managed Apache Beam environment for stream (and batch) processing. Finally, <strong>BigQuery</strong> is the premier data warehouse for large-scale analytics."
  },
  {
    "id": "q2",
    "question": "Your team needs to migrate a 15 TB on-premises Hadoop cluster to the cloud. The main goal is to minimize operational overhead and separate storage from compute. What is the most cost-effective and efficient approach?",
    "options": [
      "Lift and shift the entire cluster to Compute Engine VMs.",
      "Use HDFS on a persistent Dataproc cluster.",
      "Migrate data to Cloud Storage and use ephemeral Dataproc clusters for jobs.",
      "Ingest all data into Cloud Spanner."
    ],
    "correct_answers": ["Migrate data to Cloud Storage and use ephemeral Dataproc clusters for jobs."],
    "categories": ["Data Migration", "Data Processing", "Cost Optimization"],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "Storing data in <strong>Cloud Storage</strong> decouples storage and compute, which is highly cost-effective. You can then spin up <strong>ephemeral Dataproc clusters</strong> only when you need to run jobs, paying for compute resources on-demand. This is a core best practice for Hadoop/Spark on GCP."
  },
  {
    "id": "q3",
    "question": "You need to ensure data quality and transform CSV files before loading them into BigQuery. The files may contain corrupted rows. Which services are best suited for building a robust, serverless ETL pipeline to handle this? (Select all that apply)",
    "options": [
      "Cloud Functions triggered by Cloud Storage events.",
      "A Dataflow pipeline that validates and transforms data.",
      "A Dataprep by Trifacta job.",
      "Loading data directly and cleaning it with SQL in BigQuery."
    ],
    "correct_answers": ["Cloud Functions triggered by Cloud Storage events.", "A Dataflow pipeline that validates and transforms data.", "A Dataprep by Trifacta job."],
    "categories": ["ETL", "Data Quality", "Data Processing"],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "Multiple serverless options exist. <strong>Cloud Functions</strong> can handle lightweight, event-driven transformations. <strong>Dataflow</strong> is ideal for complex, large-scale transformations and can route bad records to a dead-letter queue. <strong>Dataprep</strong> provides a visual interface for building data cleaning and transformation rules, which then runs a Dataflow job under the hood. While cleaning in BigQuery is possible, it's often better to clean the data *before* loading to ensure the data warehouse stays clean."
  },
  {
    "id": "q4",
    "question": "Let's say you have been using BigTable instance with HDD as storage type. You want to increase the performance of the instance by changing the storage type to SSD. You want to make sure the data will not be lost. How can you achieve that?",
    "options": [
      "From the Google Cloud console UI, you can switch the storage type from HDD to SDD. Data will be moved to a new storage type. Instance will be in write-only mode by this time until the migration is complete.",
      "From Google Cloud console UI, you can switch the storage type from HDD to SDD. Data will be moved to new storage type. Instance will be inaccessible by this time until migration is complete.",
      "Export the data to Cloud Storage in Avro format using Dataflow template and import data into new BigTable instance using Dataflow GCS Avro to BigTable",
      "From Google Cloud console UI, you can switch the storage type from HDD to SDD. Data will be moved to new storage type. Instance will be in read-only mode by this time until the migration is complete."
    ],
    "correct_answers": [
      "Export the data to Cloud Storage in Avro format using Dataflow template and import data into new BigTable instance using Dataflow GCS Avro to BigTable"
    ],
    "categories": [
      "Data Migration",
      "BigTable",
      "Dataflow"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q4.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  },
  {
    "id": "q5",
    "question": "You have a BigQuery dataset named 'customers'. All tables will be tagged by using a Data Catalog tag template named 'gdpr'. The template contains one mandatory field, 'has_sensitive_data', with a boolean value. All employees must be able to do a simple search and find tables in the dataset that have either true or false in the 'has_sensitive_data' field. However, only the Human Resources (HR) group should be able to see the data inside the tables for which 'has_sensitive data' is true. You give the all employees group the bigquery.metadataViewer and bigquery.connectionUser roles on the dataset. What should you do next to minimize configuration overhead?",
    "options": [
      "Create the 'gdpr' tag template with private visibility. Assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.",
      "Create the 'gdpr' tag template with private visibility. Assign the datacatalog.tagTemplateViewer role on this tag to the all employees group, and assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.",
      "Create the 'gdpr' tag template with public visibility. Assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.",
      "Create the 'gdpr' tag template with public visibility. Assign the datacatalog.tagTemplateViewer role on this tag to the all employees group, and assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data."
    ],
    "correct_answers": [
      "Create the 'gdpr' tag template with public visibility. Assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data."
    ],
    "categories": [
      "Data Governance",
      "Security",
      "BigQuery",
      "Data Catalog"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q5.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  },
  {
    "id": "q6",
    "question": "You are monitoring your organization's data lake hosted on BigQuery. The ingestion pipelines read data from Pub/Sub and write the data into tables on BigQuery. After deploying a new version of the ingestion pipelines, the daily stored data increased by 50%. The volumes of data in Pub/Sub remained the same and only some tables had their daily partition data size doubled. You need to investigate and resolve the cause of the data increase. What should you do to investigate and fix the cause of the data increase?",
    "options": [
      "1. Check for duplicate rows in the BigQuery tables that have the daily partition data size doubled. 2. Schedule daily SQL jobs to deduplicate the affected tables. 3. Share the deduplication script with the other operational teams to reuse if this occurs to other tables.",
      "1. Check for code errors in the deployed pipelines. 2. Check for multiple writing to pipeline BigQuery sink. 3. Check for errors in Cloud Logging during the day of the release of the new pipelines. 4. If no errors, restore the BigQuery tables to their content before the last release by using time travel.",
      "1. Check for duplicate rows in the BigQuery tables that have the daily partition data size doubled. 2. Check the BigQuery Audit logs to find job IDs. 3. Use Cloud Monitoring to determine when the identified Dataflow jobs started and the pipeline code version. 4. When more than one pipeline ingests data into a table, stop all versions except the latest one.",
      "1. Roll back the last deployment. 2. Restore the BigQuery tables to their content before the last release by using time travel. 3. Restart the Dataflow jobs and replay the messages by seeking the subscription to the timestamp of the release."
    ],
    "correct_answers": [
      "1. Check for duplicate rows in the BigQuery tables that have the daily partition data size doubled. 2. Check the BigQuery Audit logs to find job IDs. 3. Use Cloud Monitoring to determine when the identified Dataflow jobs started and the pipeline code version. 4. When more than one pipeline ingests data into a table, stop all versions except the latest one."
    ],
    "categories": [
      "BigQuery",
      "Dataflow",
      "Pub/Sub",
      "Troubleshooting",
      "Data Ingestion"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q6.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q7",
    "question": "You need to create a SQL pipeline that runs an aggregate SQL transformation on a BigQuery table every two hours and appends the result to another BigQuery table. The pipeline must be configured to retry if errors occur, and it should send an email notification after three consecutive failures. What should you do to meet these requirements?",
    "options": [
      "Use the BigQueryUpsertTableOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.",
      "Use the BigQueryInsertJobOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.",
      "Create a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable email notifications.",
      "Create a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable notification to Pub/Sub topic. Use Pub/Sub and Cloud Functions to send an email after three failed executions."
    ],
    "correct_answers": [
      "Use the BigQueryInsertJobOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true."
    ],
    "categories": [
      "BigQuery",
      "Cloud Composer",
      "Orchestration",
      "Data Engineering"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q7.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q8",
    "question": "Your organization stores data in BigQuery, Pub/Sub, and a PostgreSQL instance running on Compute Engine. Due to multiple domains and diverse teams using the data, the teams have difficulty discovering the existing data assets. You need to create a solution that improves data discoverability while minimizing development and configuration efforts. What should you do to improve data discoverability across these data assets?",
    "options": [
      "Use Data Catalog to automatically catalog BigQuery datasets. Use Data Catalog APIs to manually catalog Pub/Sub topics and PostgreSQL tables.",
      "Use Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use Data Catalog APIs to manually catalog PostgreSQL tables.",
      "Use Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use custom connectors to manually catalog PostgreSQL tables.",
      "Use customer connectors to manually catalog BigQuery datasets, Pub/Sub topics, and PostgreSQL tables."
    ],
    "correct_answers": [
      "Use Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use Data Catalog APIs to manually catalog PostgreSQL tables."
    ],
    "categories": [
      "Data Catalog",
      "Data Governance",
      "BigQuery",
      "Pub/Sub",
      "PostgreSQL"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q8.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q9",
    "question": "Let’s say you work as a data engineer in an organization with a large amount of data. They use Google Big Table to store their web service’s activity logs for faster retrieval and update. What will happen if the BigTable node fails?",
    "options": [
        "None of the above",
        "Recover data from Cloud Storage when the node comes back online",
        "Data will not be lost",
        "Data will be lost"
    ],
    "correct_answers": [
        "Data will not be lost"
    ],
    "categories": [
        "BigTable",
        "Architecture",
        "Reliability"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q9.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
]