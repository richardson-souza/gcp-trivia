[
  {
    "id": "q1",
    "question": "You are running a BigQuery project using the on-demand billing model, and you are executing a change data capture (CDC) process that ingests data. The CDC process loads 1 GB of data every 10 minutes into a temporary table and performs a merge into a 10 TB target table. This process is resource-intensive and involves significant scanning. You want to explore options to implement a predictable cost model. You need to create a BigQuery reservation based on usage data gathered from BigQuery Monitoring and apply the reservation to the CDC process.",
    "options": [
      "Create a BigQuery reservation for the dataset.",
      "Create a BigQuery reservation for the job.",
      "Create a BigQuery reservation for the service account running the job.",
      "Create a BigQuery reservation for the project."
    ],
    "correct_answers": ["Create a BigQuery reservation for the project."],
    "categories": ["Data Ingestion", "Data Processing", "Analytics"],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "For real-time, large-scale scenarios, <strong>Cloud Pub/Sub</strong> is the ideal ingestion service due to its scalability and durability. <strong>Cloud Dataflow</strong> provides a managed Apache Beam environment for stream (and batch) processing. Finally, <strong>BigQuery</strong> is the premier data warehouse for large-scale analytics."
  },
  {
    "id": "q2",
    "question": "Your team needs to migrate a 15 TB on-premises Hadoop cluster to the cloud. The main goal is to minimize operational overhead and separate storage from compute. What is the most cost-effective and efficient approach?",
    "options": [
      "Lift and shift the entire cluster to Compute Engine VMs.",
      "Use HDFS on a persistent Dataproc cluster.",
      "Migrate data to Cloud Storage and use ephemeral Dataproc clusters for jobs.",
      "Ingest all data into Cloud Spanner."
    ],
    "correct_answers": ["Migrate data to Cloud Storage and use ephemeral Dataproc clusters for jobs."],
    "categories": ["Data Migration", "Data Processing", "Cost Optimization"],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "Storing data in <strong>Cloud Storage</strong> decouples storage and compute, which is highly cost-effective. You can then spin up <strong>ephemeral Dataproc clusters</strong> only when you need to run jobs, paying for compute resources on-demand. This is a core best practice for Hadoop/Spark on GCP."
  },
  {
    "id": "q3",
    "question": "You need to ensure data quality and transform CSV files before loading them into BigQuery. The files may contain corrupted rows. Which services are best suited for building a robust, serverless ETL pipeline to handle this? (Select all that apply)",
    "options": [
      "Cloud Functions triggered by Cloud Storage events.",
      "A Dataflow pipeline that validates and transforms data.",
      "A Dataprep by Trifacta job.",
      "Loading data directly and cleaning it with SQL in BigQuery."
    ],
    "correct_answers": ["Cloud Functions triggered by Cloud Storage events.", "A Dataflow pipeline that validates and transforms data.", "A Dataprep by Trifacta job."],
    "categories": ["ETL", "Data Quality", "Data Processing"],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "Multiple serverless options exist. <strong>Cloud Functions</strong> can handle lightweight, event-driven transformations. <strong>Dataflow</strong> is ideal for complex, large-scale transformations and can route bad records to a dead-letter queue. <strong>Dataprep</strong> provides a visual interface for building data cleaning and transformation rules, which then runs a Dataflow job under the hood. While cleaning in BigQuery is possible, it's often better to clean the data *before* loading to ensure the data warehouse stays clean."
  },
  {
    "id": "q4",
    "question": "Let's say you have been using BigTable instance with HDD as storage type. You want to increase the performance of the instance by changing the storage type to SSD. You want to make sure the data will not be lost. How can you achieve that?",
    "options": [
      "From the Google Cloud console UI, you can switch the storage type from HDD to SDD. Data will be moved to a new storage type. Instance will be in write-only mode by this time until the migration is complete.",
      "From Google Cloud console UI, you can switch the storage type from HDD to SDD. Data will be moved to new storage type. Instance will be inaccessible by this time until migration is complete.",
      "Export the data to Cloud Storage in Avro format using Dataflow template and import data into new BigTable instance using Dataflow GCS Avro to BigTable",
      "From Google Cloud console UI, you can switch the storage type from HDD to SDD. Data will be moved to new storage type. Instance will be in read-only mode by this time until the migration is complete."
    ],
    "correct_answers": [
      "Export the data to Cloud Storage in Avro format using Dataflow template and import data into new BigTable instance using Dataflow GCS Avro to BigTable"
    ],
    "categories": [
      "Data Migration",
      "BigTable",
      "Dataflow"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q4.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  },
  {
    "id": "q5",
    "question": "You have a BigQuery dataset named 'customers'. All tables will be tagged by using a Data Catalog tag template named 'gdpr'. The template contains one mandatory field, 'has_sensitive_data', with a boolean value. All employees must be able to do a simple search and find tables in the dataset that have either true or false in the 'has_sensitive_data' field. However, only the Human Resources (HR) group should be able to see the data inside the tables for which 'has_sensitive data' is true. You give the all employees group the bigquery.metadataViewer and bigquery.connectionUser roles on the dataset. What should you do next to minimize configuration overhead?",
    "options": [
      "Create the 'gdpr' tag template with private visibility. Assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.",
      "Create the 'gdpr' tag template with private visibility. Assign the datacatalog.tagTemplateViewer role on this tag to the all employees group, and assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.",
      "Create the 'gdpr' tag template with public visibility. Assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.",
      "Create the 'gdpr' tag template with public visibility. Assign the datacatalog.tagTemplateViewer role on this tag to the all employees group, and assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data."
    ],
    "correct_answers": [
      "Create the 'gdpr' tag template with public visibility. Assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data."
    ],
    "categories": [
      "Data Governance",
      "Security",
      "BigQuery",
      "Data Catalog"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q5.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  },
  {
    "id": "q6",
    "question": "You are monitoring your organization's data lake hosted on BigQuery. The ingestion pipelines read data from Pub/Sub and write the data into tables on BigQuery. After deploying a new version of the ingestion pipelines, the daily stored data increased by 50%. The volumes of data in Pub/Sub remained the same and only some tables had their daily partition data size doubled. You need to investigate and resolve the cause of the data increase. What should you do to investigate and fix the cause of the data increase?",
    "options": [
      "1. Check for duplicate rows in the BigQuery tables that have the daily partition data size doubled. 2. Schedule daily SQL jobs to deduplicate the affected tables. 3. Share the deduplication script with the other operational teams to reuse if this occurs to other tables.",
      "1. Check for code errors in the deployed pipelines. 2. Check for multiple writing to pipeline BigQuery sink. 3. Check for errors in Cloud Logging during the day of the release of the new pipelines. 4. If no errors, restore the BigQuery tables to their content before the last release by using time travel.",
      "1. Check for duplicate rows in the BigQuery tables that have the daily partition data size doubled. 2. Check the BigQuery Audit logs to find job IDs. 3. Use Cloud Monitoring to determine when the identified Dataflow jobs started and the pipeline code version. 4. When more than one pipeline ingests data into a table, stop all versions except the latest one.",
      "1. Roll back the last deployment. 2. Restore the BigQuery tables to their content before the last release by using time travel. 3. Restart the Dataflow jobs and replay the messages by seeking the subscription to the timestamp of the release."
    ],
    "correct_answers": [
      "1. Check for duplicate rows in the BigQuery tables that have the daily partition data size doubled. 2. Check the BigQuery Audit logs to find job IDs. 3. Use Cloud Monitoring to determine when the identified Dataflow jobs started and the pipeline code version. 4. When more than one pipeline ingests data into a table, stop all versions except the latest one."
    ],
    "categories": [
      "BigQuery",
      "Dataflow",
      "Pub/Sub",
      "Troubleshooting",
      "Data Ingestion"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q6.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q7",
    "question": "You need to create a SQL pipeline that runs an aggregate SQL transformation on a BigQuery table every two hours and appends the result to another BigQuery table. The pipeline must be configured to retry if errors occur, and it should send an email notification after three consecutive failures. What should you do to meet these requirements?",
    "options": [
      "Use the BigQueryUpsertTableOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.",
      "Use the BigQueryInsertJobOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.",
      "Create a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable email notifications.",
      "Create a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable notification to Pub/Sub topic. Use Pub/Sub and Cloud Functions to send an email after three failed executions."
    ],
    "correct_answers": [
      "Use the BigQueryInsertJobOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true."
    ],
    "categories": [
      "BigQuery",
      "Cloud Composer",
      "Orchestration",
      "Data Engineering"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q7.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q8",
    "question": "Your organization stores data in BigQuery, Pub/Sub, and a PostgreSQL instance running on Compute Engine. Due to multiple domains and diverse teams using the data, the teams have difficulty discovering the existing data assets. You need to create a solution that improves data discoverability while minimizing development and configuration efforts. What should you do to improve data discoverability across these data assets?",
    "options": [
      "Use Data Catalog to automatically catalog BigQuery datasets. Use Data Catalog APIs to manually catalog Pub/Sub topics and PostgreSQL tables.",
      "Use Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use Data Catalog APIs to manually catalog PostgreSQL tables.",
      "Use Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use custom connectors to manually catalog PostgreSQL tables.",
      "Use customer connectors to manually catalog BigQuery datasets, Pub/Sub topics, and PostgreSQL tables."
    ],
    "correct_answers": [
      "Use Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use Data Catalog APIs to manually catalog PostgreSQL tables."
    ],
    "categories": [
      "Data Catalog",
      "Data Governance",
      "BigQuery",
      "Pub/Sub",
      "PostgreSQL"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q8.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q9",
    "question": "Let’s say you work as a data engineer in an organization with a large amount of data. They use Google Big Table to store their web service’s activity logs for faster retrieval and update. What will happen if the BigTable node fails?",
    "options": [
        "None of the above",
        "Recover data from Cloud Storage when the node comes back online",
        "Data will not be lost",
        "Data will be lost"
    ],
    "correct_answers": [
        "Data will not be lost"
    ],
    "categories": [
        "BigTable",
        "Architecture",
        "Reliability"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q9.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q10",
    "question": "Let’s say a social media platform stores various details of their platform users such as session login time, URLs visited, activities on the platform and other logs. With GDPR (General Data Protection Regulation) compliance to be officially implemented, the platform now allows users to download their activity logs from their profile settings which they can click a button to call an API to generate a full report. Recently, users are complaining timeouts after 60 seconds of requesting to download their activity logs at peak hours when the platform has the most traffic. They have to try for several minutes or even hours for the API to return their report available for download. How can you solve this issue?",
    "options": [
      "Use Pub/Sub to receive requests for activity logs from users. Deploy a Cloud Function with a Pub/Sub trigger to generate the reports and store them in a GCS bucket. Then, send temporary download links to the users via email.",
      "Migrate data source to Cloud Spanner for horizontal scaling to avoid query timeouts.",
      "Build a Dataflow pipeline to generate daily reports of users’ activity logs. Users can download those daily reports whenever they want to.",
      "Increase timeout for API at peak times to 120 seconds. If it keeps failing, try increasing the timeout until the issue is resolved."
    ],
    "correct_answers": [
      "Use Pub/Sub to receive requests for activity logs from users. Deploy a Cloud Function with a Pub/Sub trigger to generate the reports and store them in a GCS bucket. Then, send temporary download links to the users via email."
    ],
    "categories": [
      "Pub/Sub",
      "Cloud Functions",
      "GCS",
      "Asynchronous Processing",
      "Scalability"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q10.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q11",
    "question": "A new data scientist named Ava has joined TechNova Solutions. She needs to carry out complex analyses on large datasets stored in Google Cloud Storage and a Cassandra cluster running on Google Compute Engine. Her primary goal is to generate labeled datasets for machine learning and perform some data visualizations. Ava mentions that her current laptop lacks the computing power required and is causing delays. You want to assist her in completing her tasks efficiently. What should you do?",
    "options": [
      "Set up and run a local Jupyter notebook on her laptop.",
      "Provide her with access to Google Cloud Shell.",
      "Launch a visualization application on a virtual machine in Google Compute Engine.",
      "Deploy Google Cloud Datalab on a virtual machine in Google Compute Engine."
    ],
    "correct_answers": [
      "Deploy Google Cloud Datalab on a virtual machine in Google Compute Engine."
    ],
    "categories": [
      "Data Science",
      "Machine Learning",
      "Big Data",
      "Compute Engine",
      "Data Visualization"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q11.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q12",
    "question": "You are rolling out 10,000 IoT sensors across FusionWare Inc.’s global warehouses to monitor temperature readings. These devices will generate high volumes of data, and your goal is to process, store, and analyze this data in real time. What is the most suitable approach?",
    "options": [
      "Ingest the data into Cloud Datastore, then export it to BigQuery for analysis.",
      "Stream the data through Cloud Pub/Sub, use Cloud Dataflow for processing, and store the results in BigQuery.",
      "Store the data in Cloud Storage, and spin up an Apache Hadoop cluster on Cloud Dataproc whenever analysis is needed.",
      "Export logs in batches to Cloud Storage, spin up a Cloud SQL instance, import the data, and perform analysis when required."
    ],
    "correct_answers": [
      "Stream the data through Cloud Pub/Sub, use Cloud Dataflow for processing, and store the results in BigQuery."
    ],
    "categories": [
      "IoT",
      "Real-time Processing",
      "Data Ingestion",
      "Data Processing",
      "Analytics"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q12.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q13",
    "question": "You’ve recently spent several days importing data from CSV files into a BigQuery table named CLICK_STREAM_LOGS for SkyMart Corp. The table includes a column named dt, which currently stores click event timestamps in epoch format as strings. For simplicity, you originally defined all fields as STRING data types. Now, you need to calculate web session durations based on these click events, and you want to convert dt to the TIMESTAMP type. Your goal is to keep the migration process as minimal as possible while ensuring future queries remain efficient. What should you do?",
    "options": [
      "Drop the CLICK_STREAM_LOGS table, recreate it with dt as a TIMESTAMP column, and reload all data.",
      "Add a new column ts of TIMESTAMP type to CLICK_STREAM_LOGS, populate it using converted values from dt, and use ts for future queries.",
      "Create a view CLICK_STREAM_VIEW that casts dt to TIMESTAMP on-the-fly. Use this view instead of the original table for future queries.",
      "Add two columns to CLICK_STREAM_LOGS: a TIMESTAMP column ts and a BOOLEAN column is_new. Reload all data in append mode, setting is_new to true. In future queries, reference ts and filter using is_new = true."
    ],
    "correct_answers": [
      "Add a new column ts of TIMESTAMP type to CLICK_STREAM_LOGS, populate it using converted values from dt, and use ts for future queries."
    ],
    "categories": [
      "BigQuery",
      "Data Migration",
      "Schema Evolution",
      "Data Types",
      "Query Optimization"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q13.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q14",
    "question": "Let’s say the data scientists at your company have successfully built a deep neural network machine learning model to detect car plate numbers entering and exiting a parking lot of a high-rise condominium. The model was built using Tensorflow and the model was exported as SavedModel. As a data engineer, you are assigned to deploy their model. The company is using Google Cloud for its project. Which approach is best for deploying the detection model?",
    "options": [
      "Deploy the model to Google Kubernetes Engine after wrapping SavedModel as docker image and uploading it to Google Container Registry.",
      "Deploy the model exported as SavedModel directly to Vertex AI in GCP.",
      "Upload SavedModel object to Google Storage. Use Dataproc with Spark ML to use the model by accessing it using Google Storage Connector.",
      "Deploy the model to ML module in GCP after asking the data science team to convert the model to binary format using PyTorch."
    ],
    "correct_answers": [
      "Deploy the model exported as SavedModel directly to Vertex AI in GCP."
    ],
    "categories": [
      "Machine Learning",
      "Model Deployment",
      "TensorFlow",
      "Vertex AI"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q14.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q15",
    "question": "You are working at DataLoop Systems, and you need to monitor BigQuery usage through Google Cloud Logging (formerly Stackdriver Logging). Specifically, you want to receive real-time alerts through your monitoring system whenever new rows are inserted into a specific BigQuery table via an insert job. However, you want to avoid alerts for insert operations on any other tables. What should you do?",
    "options": [
      "Use the Cloud Logging API to list all logs, and apply an advanced filter on the client side.",
      "Go to the Cloud Logging interface and create a log sink that exports logs to BigQuery.",
      "In the Cloud Logging interface, set up a log sink that exports to Cloud Pub/Sub, then have your monitoring tool subscribe to the topic.",
      "Use the Cloud Logging API to create a project-level sink with an advanced log filter that exports to Cloud Pub/Sub, and have your monitoring system subscribe to that topic."
    ],
    "correct_answers": [
      "Use the Cloud Logging API to create a project-level sink with an advanced log filter that exports to Cloud Pub/Sub, and have your monitoring system subscribe to that topic."
    ],
    "categories": [
      "Monitoring",
      "Logging",
      "BigQuery",
      "Pub/Sub",
      "Real-time Alerts"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q15.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q16",
    "question": "Let’s say you are building a machine learning classification model using TensorFlow. You trained the model by using 70% of the total set available for training, validation and testing. After testing the model, AUC returned from the test results was 0.68. The main issue here is due to overfitting. You want to increase the AUC for better accuracy of results. What should you do?",
    "options": [
      "Reduce regularization.",
      "Increase regularization.",
      "Increase feature parameters.",
      "Reduce samples used for training."
    ],
    "correct_answers": [
      "Increase regularization."
    ],
    "categories": [
      "Machine Learning",
      "TensorFlow",
      "Model Training",
      "Overfitting",
      "Regularization"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q16.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q17",
    "question": "You are managing a high-sensitivity project for NexaCore Analytics, which involves processing private user data. The project is hosted internally on Google Cloud Platform (GCP). To assist with a complex transformation in your Cloud Dataflow pipeline, an external developer has been brought in for temporary collaboration. You must ensure that user privacy is protected during their involvement. What is the most appropriate action to take?",
    "options": [
      "Assign the Viewer role to the consultant at the project level.",
      "Grant the Cloud Dataflow Developer role to the consultant for the project.",
      "Set up a service account and let the consultant log in using its credentials.",
      "Provide the consultant with access to an anonymized version of the dataset located in a separate GCP project."
    ],
    "correct_answers": [
      "Provide the consultant with access to an anonymized version of the dataset located in a separate GCP project."
    ],
    "categories": [
      "Data Security",
      "Data Privacy",
      "Dataflow",
      "IAM",
      "GCP Best Practices"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q17.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q18",
    "question": "Let’s say your team decided to use BigTable for storing event data. The engineer responsible of launching and testing the instance has reported a slower performance than expected by Google Cloud documentation. Which of the following could be a factor for the slow performance?",
    "options": [
      "The rows in the tables have large data size.",
      "The instance doesn’t have enough nodes.",
      "Test data size is over 300GB.",
      "The rows in the tables tested contain high number of cells."
    ],
    "correct_answers": [
      "The rows in the tables have large data size.",
      "The instance doesn’t have enough nodes.",
      "The rows in the tables tested contain high number of cells."
    ],
    "categories": [
      "BigTable",
      "Performance Tuning",
      "Database",
      "Troubleshooting"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q18.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q19",
    "question": "Let’s say you have several Data Studio reports reading from BigQuery. Those reports are used to visualize several metrics for marketing team. Data visualized is updated only once a day. You notice that reports running queries on BigQuery are not free and they cost for each query. You want to control and minimize the costs caused by frequent queries coming from Data Studio dashboards.What should you do?",
    "options": [
      "Export data as CSV files to Google Storage every 24 hours and change reports data source to read from those files.",
      "Grant owner credentials for the reports on BigQuery datasets and enable caching.",
      "Configure reports data sources to update data every 24 hours only.",
      "Enable caching on reports for reading from BigQuery. No need to change the credentials."
    ],
    "correct_answers": [
      "Enable caching on reports for reading from BigQuery. No need to change the credentials."
    ],
    "categories": [
      "BigQuery",
      "Data Studio",
      "Cost Optimization",
      "Analytics"
    ],
    "is_verified": true,
    "is_ai_generated": false,
    "explanation": "<a href=\"explanations/q19.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  }
  ,{
    "id": "q20",
    "question": "A large retail company manages a 100 TB `sales_transactions` table in BigQuery. The table is partitioned by `transaction_date`. Analysts frequently run queries filtering by the `store_id` (an INT64 column with high cardinality) and applying aggregations. They report that despite the date partitioning, these queries remain slow and expensive because the filter on `store_id` is highly selective but requires scanning large amounts of data within each partition. Which action should the data engineer take to optimize performance and minimize query costs for queries that frequently filter on `store_id`?",
    "options": [
      "Use integer range partitioning on the `store_id` column instead of partitioning by `transaction_date`.",
      "Change the table from on-demand pricing to BigQuery Editions (capacity-based pricing) to ensure dedicated slots.",
      "Add CLUSTER BY (`store_id`) to the existing date-partitioned table.",
      "Create a daily scheduled query to export highly aggregated sales metrics by date and store ID to a new, smaller destination table."
    ],
    "correct_answers": [
      "Add CLUSTER BY (`store_id`) to the existing date-partitioned table."
    ],
    "categories": [
      "BigQuery",
      "Cost Optimization",
      "Performance Tuning",
      "Architecture"
    ],
    "is_verified": true,
    "is_ai_generated": true,
    "explanation": "<a href=\"explanations/q20.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  },
  {
    "id": "q21",
    "question": "A company stores sensitive customer PII, including credit card numbers (`cc_number`), in a large BigQuery table. Data scientists require access to most columns for machine learning model training but must be strictly prevented from accessing the `cc_number` column. The solution must be centrally governed and scalable across multiple datasets. What is the most secure and scalable approach to restrict the Data Scientists' access to the `cc_number` column?",
    "options": [
      "Grant the Data Scientists the `roles/bigquery.dataViewer` role and use an authorized view that executes `SELECT * EXCEPT(cc_number)` to provide access.",
      "Apply a Data Masking Policy to the `cc_number` column that replaces the values with NULL, ensuring users without specific permissions see no data.",
      "Create a Taxonomy and Policy Tag (e.g., PII:CreditCard). Apply the Policy Tag to the `cc_number` column, and ensure Data Scientists are not granted permission to view data associated with that tag.",
      "Use BigQuery Row-Level Security policies to filter out all rows where the `cc_number` column is populated, preventing access to the data."
    ],
    "correct_answers": [
      "Create a Taxonomy and Policy Tag (e.g., PII:CreditCard). Apply the Policy Tag to the `cc_number` column, and ensure Data Scientists are not granted permission to view data associated with that tag."
    ],
    "categories": [
      "BigQuery",
      "Security",
      "Data Governance",
      "Data privacy",
      "IAM"
    ],
    "is_verified": true,
    "is_ai_generated": true,
    "explanation": "<a href=\"explanations/q21.html\" target=\"_blank\" class=\"text-blue-500 hover:underline\">View Detailed Explanation</a>"
  },
  {
    "id": "q22",
    "question": "A financial application generates high-volume, continuous event data in JSON format that needs to be ingested into BigQuery. The data must be transformed (schema flattened and validated) before loading, and must be queryable by analysts with sub-second latency for real-time risk dashboards. The current processing rate is approximately 100,000 events per second. Which architecture represents the most efficient and scalable pattern for ingesting, transforming, and analyzing this streaming data in BigQuery?",
    "options": [
      "Use Pub/Sub for ingestion, transform the data using a Cloud Dataflow streaming pipeline built with Apache Beam, and stream the results directly into BigQuery.",
      "Stream raw JSON data directly into BigQuery using the Streaming API, and then use SQL Multi-statement queries with views to perform transformations on the raw data.",
      "Stream the data into Cloud Bigtable using a Dataflow sink, and use BigQuery Federated Queries for real-time dashboards.",
      "Use Cloud Composer to schedule hourly ETL jobs via Dataproc Serverless for Spark, reading from Pub/Sub and writing transformed data to BigQuery."
    ],
    "correct_answers": [
      "Use Pub/Sub for ingestion, transform the data using a Cloud Dataflow streaming pipeline built with Apache Beam, and stream the results directly into BigQuery."
    ],
    "categories": [
      "BigQuery",
      "Dataflow",
      "Pub/Sub",
      "ETL",
      "Real-time Alerts"
    ],
    "is_verified": true,
    "is_ai_generated": true,
    "explanation": ""
  },
  {
    "id": "q23",
    "question": "A data engineering team is optimizing query performance for existing BI dashboards that frequently join a 5 TB `Fact_Events` table with a 50 GB `Dim_Attributes` table using a non-integer foreign key. These joins result in high query execution times dominated by the shuffle stage. The goal is to maximize performance for read operations with minimum computational overhead. Based on BigQuery modeling best practices for minimizing shuffle and I/O costs, what is the best architectural change to implement?",
    "options": [
      "Keep the tables separate but apply clustering to both tables using the foreign key column to improve join efficiency.",
      "Denormalize the data by embedding the `Dim_Attributes` fields into the `Fact_Events` table using nested and repeated fields.",
      "Refactor the foreign key column in both tables from STRING to INT64 to optimize comparison performance during the JOIN operation.",
      "Convert the dimension table into a BigQuery Materialized View to ensure the attribute lookups are cached."
    ],
    "correct_answers": [
      "Denormalize the data by embedding the `Dim_Attributes` fields into the `Fact_Events` table using nested and repeated fields."
    ],
    "categories": [
      "BigQuery",
      "Data Processing",
      "Architecture",
      "Performance Tuning",
      "Data Modeling"
    ],
    "is_verified": true,
    "is_ai_generated": true,
    "explanation": ""
  },
  {
    "id": "q24",
    "question": "An analyst in Project A (`europe-west1`) needs to run an ad-hoc SQL query against a 50 TB table located in Project B (`us-central1`). Project A uses BigQuery On-Demand pricing. The analyst has been granted the necessary `bigquery.dataViewer` role on the table in Project B. When running the query, the analyst encounters high egress charges. What configuration setting must the analyst change to execute this cross-project query while minimizing cross-region data egress charges?",
    "options": [
      "Set the BigQuery Data Transfer Service to automatically move the table from Project B's region to Project A's region before querying.",
      "In Project A, enable BigQuery BI Engine reservation capacity to accelerate the query and mitigate transfer latency.",
      "The analyst must set the query job's processing location (or region) to `us-central1` (Project B's region).",
      "Use a BigLake external table definition in Project A, pointing to the original table data in Project B."
    ],
    "correct_answers": [
      "The analyst must set the query job's processing location (or region) to `us-central1` (Project B's region)."
    ],
    "categories": [
      "BigQuery",
      "Cost Optimization",
      "Architecture",
      "Data Migration",
      "GCP Best Practices"
    ],
    "is_verified": true,
    "is_ai_generated": true,
    "explanation": ""
  }
]